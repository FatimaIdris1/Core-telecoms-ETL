# Coretelecoms ETL Data Pipeline

## Overview

CoreTelecoms, a major telecom provider in the United States, is facing increasing customer churn due to unresolved and poorly managed complaints. The organization receives thousands of complaints daily from multiple sources such as call logs, social media, website forms, and customer care centers. However, due to inconsistent formats, siloed teams, and manual reporting processes, insights are delayed and customer retention continues to decline.

This project implements a unified data engineering pipeline that automates ingestion, storage, and processing of all customer complaint sources into Snowflake, enabling analytics and reporting from a single trusted dataset. The pipeline eliminates manual bottlenecks and creates a foundation for real-time monitoring, customer service optimization, and churn reduction.

---

## Data Sources

| Source                  | Description                                                                     | Format        | Location           | Frequency |
| ----------------------- | ------------------------------------------------------------------------------- | ------------- | ------------------ | --------- |
| Customers               | Customer information: ID, name, contact, location, etc.                         | CSV           | AWS S3             | Static    |
| Agents                  | Customer care agent lookup table                                                | Google Sheets | Google Drive       | Static    |
| Call Center Logs        | Daily logs with complaint type, agent handling, resolution status, and duration | CSV           | AWS S3             | Daily     |
| Social Media            | Online complaints containing customer/agent-related metadata                    | JSON          | AWS S3             | Daily     |
| Website Complaint Forms | Customer-submitted forms containing complaint records                           | Table         | AWS Postgres (RDS) | Daily     |

### Processing Behavior

Daily datasets uploaded to S3 may contain multiple files per source. Files are merged using append logic and then loaded into Snowflake. Static dataset sources (Google Sheets and Customers CSV) are loaded once via dedicated tasks in Airflow.

---

## Technologies Used

### Cloud and Storage

* AWS S3
* AWS IAM
* AWS SSM Parameter Store
* AWS Postgres (Transactional DB)

### Data Warehouse

* Snowflake

### Data Orchestration

* Apache Airflow

### Infrastructure Automation

* Terraform

### Programming and Libraries

* Python
* boto3
* pandas
* pyarrow
* snowflake-connector-python

---

## Pipeline Features

* Automatic folder-level processing of S3 data
* Daily scheduled ingestion orchestrated with Airflow
* Incremental loading and prevention of duplicate loads
* Append logic for merging daily batch files
* Multi-source extraction (S3, Google Sheets, Postgres)
* Automated table creation in Snowflake
* Schema inference for dynamic loading
* Fully automated IAM and Snowflake configuration with Terraform
* Logging and monitoring using Airflow

---

## Architecture Summary

Although a diagram will be added later, the pipeline currently follows the sequence below:

1. Extract data from:

   * S3 folders containing CSV and JSON data
   * Google Sheets
   * Postgres transactional database
2. Apply append logic for daily incremental data
3. Detect and skip previously processed files
4. Load curated files into Snowflake tables
5. Execute transformations before final warehouse ingestion

---

## Snowflake Configuration

Deployed through Terraform:

* **Database:** `CORETELECOM_DB`
* **Schema:** `CORETELECOM_STAGING`
* **Warehouse:** `COMPUTE_WH`
* **Storage Integration:** `integration`
* **AWS IAM Role:** `snowflake_user_role`

The Snowflake stage, integration, roles, and schema configurations are all generated by Terraform located within `aws_infrastructure/snowflake.tf`.

---

## Airflow Data Pipeline

### DAG Name

`core_etl_pipeline`

### Primary Tasks

```
single_load
incremental_load
transformations
snowflake_load
```

### Directory Structure

```
Airflow/
│── dags/
│   └── pipeline.py
│
│── plugins/
│   ├── google_sheet_load.py
│   ├── customer_load.py
│   ├── postgres_extract.py
│   ├── s3_incremental_copy.py
│   ├── s3_to_snowflake.py
│   └── utilities.py
```

---

## Terraform Infrastructure Layout

```
aws_infrastructure/
│── main.tf
│── iam.tf
│── s3_buckets.tf
│── secret_manager.tf
│── security_groups.tf
│── subnets.tf
│── subnet_groups.tf
│── snowflake.tf
```

This section provisions the entire environment including VPC networking, IAM roles, secret storage, S3 buckets, PostgreSQL access, and Snowflake cloud resources.

---


## dbt Models (Facts and Dimensions)

The analytics layer of this project is fully managed in dbt. After ingestion into `CORETELECOM_STAGING`, dbt transforms the raw data into clean, analysis-ready tables under `CORETELECOM_TRANSFORMED`.

All models are in:

```
dbt/models/
│── facts/
│── dimensions/
```

You can open each file above to view the transformation logic.

---

#### Fact Tables (Event-Driven Complaint Records)

Located in: `dbt/models/facts/`

These tables represent **customer complaint events across different channels**. They grow continuously and reference customers and agents using foreign keys. Each fact file contains join logic, standardization, and cleanup of staged data.

| File                               | Resulting Table                | What It Represents                                                                                          |
| ---------------------------------- | ------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| `fact_call_center_logs.sql`        | `fact_call_center_logs`        | Phone-based customer complaints, including agent handling, duration, resolution status, and complaint type. |
| `fact_social_media_complaints.sql` | `fact_social_media_complaints` | Complaints extracted from social platforms with sentiment and user details.                                 |
| `fact_web_complaints.sql`          | `fact_web_complaints`          | Complaints submitted via website forms stored in Postgres, including request category and response status.  |

Each fact table supports KPIs such as complaints per channel, average resolution time, customer churn risk, and agent performance.

---

#### Dimension Tables (Lookup and Reference Data)

Located in: `dbt/models/dimensions/`

These tables hold **customer and agent descriptive information** that provides context for the facts. They change slowly compared to fact tables.

| File                | Resulting Table | What It Contains                                                                              |
| ------------------- | --------------- | --------------------------------------------------------------------------------------------- |
| `dim_customers.sql` | `dim_customers` | Clean customer demographics: standardized names, DOB, gender, signup info, and IDs.           |
| `dim_agents.sql`    | `dim_agents`    | Standardized customer care agent data: name, experience, state, and support role information. |

Dimensions allow grouping and filtering of facts, for example tracking complaints by gender, location, or agent experience.

---

### CI/CD Pipeline (How Code Is Validated and Deployed)

This project follows a **DataOps CI/CD approach**, meaning infrastructure, ETL, and analytics models are all version-controlled, tested, and deployed automatically.

| Component                         | CI (Checks Before Merge)                                    | CD (Production Deployment)            | Output                       |
| --------------------------------- | ----------------------------------------------------------- | ------------------------------------- | ---------------------------- |
| Terraform (AWS + Snowflake Infra) | Validate configuration and preview changes (`plan`)         | Provision infrastructure (`apply`)    | MWAA, IAM roles, S3 buckets  |
| Airflow (DAGs + Plugins)          | Lint and test Python DAGs and extract code                  | Upload to MWAA-linked S3 bucket       | Updated pipeline execution   |
| dbt (Warehouse Models)            | Validate SQL + run schema tests (`dbt test`, `dbt compile`) | Build models in Snowflake (`dbt run`) | Updated facts and dimensions |

---

### How the Above CI/CD Works

**Terraform**

* Code is checked before deployment to avoid misconfigured cloud resources.
* Only approved plans are deployed, creating infrastructure such as MWAA, IAM, S3, and Snowflake roles.

**Airflow**

* Python extraction logic is tested before merging.
* Once deployed, the DAGs automatically run in MWAA with no manual upload needed.

**dbt**

* Every schema change and model update is validated before deployment.
* When merged, dbt builds fresh fact and dimension tables in Snowflake.

---

## **Docker & Docker Compose Overview**

This setup uses a custom Docker image (coretelecoms-etl:latest) and docker-compose to provide a full ETL and data workflow environment. Airflow orchestrates pipelines, dbt builds Snowflake models, and Terraform manages infrastructure, all in isolated containers with code synced from your host machine.

### **Docker Image**

* Contains **Airflow**, **Python dependencies**, **dbt**, and **ETL scripts**.
* Provides a self-contained, reproducible environment for ETL workflows.

---

### **Docker Compose Services**

1. **Airflow (`airflow`)**

   * Runs webserver and scheduler.
   * Orchestrates ETL tasks: S3 copy, Postgres extract, transformations, Snowflake load.
   * Mounts DAGs, plugins, dbt project, and Terraform folders.
   * Exposes UI at `localhost:8080`.
   * Uses Snowflake credentials from environment variables.

2. **Terraform (`terraform`)**

   * Runs Terraform commands in a separate container.
   * Mounts `aws_infrastructure` folder.
   * Commands:

     ```bash
     docker-compose exec terraform terraform plan
     docker-compose exec terraform terraform apply
     ```

3. **dbt (`dbt`)**

   * Runs dbt commands in a separate container.
   * Mounts `dbt` folder.
   * Uses Snowflake credentials from environment variables.
   * Commands:

     ```bash
     docker-compose exec dbt dbt run --project-dir /opt/project/dbt
     ```

---

### **Workflow**

* Airflow orchestrates ETL pipelines.
* DAGs extract from S3/Postgres, transform, and load to Snowflake.
* dbt builds Snowflake models.
* Terraform manages AWS infrastructure.
* Volumes sync local code with containers.

---

### **Usage**

Start services:

```bash
docker-compose up -d
```

Access Airflow: [http://localhost:8080](http://localhost:8080)

Run dbt:

```bash
docker-compose exec dbt dbt run --project-dir /opt/project/dbt
```

Run Terraform:

```bash
docker-compose exec terraform terraform plan
docker-compose exec terraform terraform apply
```

---

## Summary

This project delivers a complete, production-ready data pipeline for processing telecom customer complaints from multiple sources into a unified Snowflake warehouse. Using Airflow for orchestration, Python for extraction, Snowflake for scalable analytics storage, and dbt for modeling, the pipeline centralizes raw data into reliable fact and dimension tables. Terraform automates and provisions all cloud and warehouse resources, ensuring consistency and reproducibility across environments.

The solution enables:

* Automated daily ingestion from S3, Google Sheets, and Postgres
* Incremental and duplicate-safe loading using append logic
* Standardized transformations through dbt facts and dimensions
* Automated schema validation and testing before deployment
* End-to-end CI/CD for infrastructure, ETL code, and analytics models

All components of the pipeline can be traced directly from the folders listed above, allowing full visibility into extraction scripts, infrastructure definitions, and dbt transformations. This makes the project modular, scalable, and ready for real telecom analytics workloads.

The Docker setup uses a **custom Docker image** (`coretelecoms-etl:latest`) and **docker-compose** to provide a complete, containerized environment for the pipeline. Airflow orchestrates the ETL workflows and manages task scheduling, dbt builds and tests Snowflake models, and Terraform handles cloud infrastructure provisioning. Each service runs in its own isolated container, ensuring consistency, reproducibility, and separation of concerns, while local project files are synced with the containers to keep code up-to-date. This setup enables development, testing, and deployment of the entire data workflow in a controlled, integrated environment.
