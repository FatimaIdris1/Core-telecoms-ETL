# Coretelecoms ETL Data Pipeline

## Overview

CoreTelecoms, a major telecom provider in the United States, is facing increasing customer churn due to unresolved and poorly managed complaints. The organization receives thousands of complaints daily from multiple sources such as call logs, social media, website forms, and customer care centers. However, due to inconsistent formats, siloed teams, and manual reporting processes, insights are delayed and customer retention continues to decline.

This project implements a unified data engineering pipeline that automates ingestion, storage, and processing of all customer complaint sources into Snowflake, enabling analytics and reporting from a single trusted dataset. The pipeline eliminates manual bottlenecks and creates a foundation for real-time monitoring, customer service optimization, and churn reduction.

---

## Data Sources

| Source                  | Description                                                                     | Format        | Location           | Frequency |
| ----------------------- | ------------------------------------------------------------------------------- | ------------- | ------------------ | --------- |
| Customers               | Customer information: ID, name, contact, location, etc.                         | CSV           | AWS S3             | Static    |
| Agents                  | Customer care agent lookup table                                                | Google Sheets | Google Drive       | Static    |
| Call Center Logs        | Daily logs with complaint type, agent handling, resolution status, and duration | CSV           | AWS S3             | Daily     |
| Social Media            | Online complaints containing customer/agent-related metadata                    | JSON          | AWS S3             | Daily     |
| Website Complaint Forms | Customer-submitted forms containing complaint records                           | Table         | AWS Postgres (RDS) | Daily     |

### Processing Behavior

Daily datasets uploaded to S3 may contain multiple files per source. Files are merged using append logic and then loaded into Snowflake. Static dataset sources (Google Sheets and Customers CSV) are loaded once via dedicated tasks in Airflow.

---

## Technologies Used

### Cloud and Storage

* AWS S3
* AWS IAM
* AWS SSM Parameter Store
* AWS Postgres (Transactional DB)

### Data Warehouse

* Snowflake

### Data Orchestration

* Apache Airflow

### Infrastructure Automation

* Terraform

### Programming and Libraries

* Python
* boto3
* pandas
* pyarrow
* snowflake-connector-python

---

## Pipeline Features

* Automatic folder-level processing of S3 data
* Daily scheduled ingestion orchestrated with Airflow
* Incremental loading and prevention of duplicate loads
* Append logic for merging daily batch files
* Multi-source extraction (S3, Google Sheets, Postgres)
* Automated table creation in Snowflake
* Schema inference for dynamic loading
* Fully automated IAM and Snowflake configuration with Terraform
* Logging and monitoring using Airflow

---

## Architecture Summary

Although a diagram will be added later, the pipeline currently follows the sequence below:

1. Extract data from:

   * S3 folders containing CSV and JSON data
   * Google Sheets
   * Postgres transactional database
2. Apply append logic for daily incremental data
3. Detect and skip previously processed files
4. Load curated files into Snowflake tables
5. Execute transformations before final warehouse ingestion

---

## Snowflake Configuration

Deployed through Terraform:

* **Database:** `CORETELECOM_DB`
* **Schema:** `CORETELECOM_STAGING`
* **Warehouse:** `COMPUTE_WH`
* **Storage Integration:** `integration`
* **AWS IAM Role:** `snowflake_user_role`

The Snowflake stage, integration, roles, and schema configurations are all generated by Terraform located within `aws_infrastructure/snowflake.tf`.

---

## Airflow Data Pipeline

### DAG Name

`core_etl_pipeline`

### Primary Tasks

```
single_load
incremental_load
transformations
snowflake_load
```

### Directory Structure

```
Airflow/
│── dags/
│   └── pipeline.py
│
│── plugins/
│   ├── google_sheet_load.py
│   ├── customer_load.py
│   ├── postgres_extract.py
│   ├── s3_incremental_copy.py
│   ├── s3_to_snowflake.py
│   └── utilities.py
```

---

## Terraform Infrastructure Layout

```
aws_infrastructure/
│── main.tf
│── iam.tf
│── s3_buckets.tf
│── secret_manager.tf
│── security_groups.tf
│── subnets.tf
│── subnet_groups.tf
│── snowflake.tf
```

This section provisions the entire environment including VPC networking, IAM roles, secret storage, S3 buckets, PostgreSQL access, and Snowflake cloud resources.

---

## Future Improvements

* Add an architecture diagram for visual representation
* Implement quality checks using Great Expectations or dbt tests
* Implement CDC for real-time ingestion of Postgres complaint tables
* Add orchestration error alerts via Slack or email

