version: "3.9"

services:

  airflow:
    image: coretelecoms-etl:latest
    container_name: coretelecoms-airflow
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 60
      SNOWFLAKE_ACCOUNT: ${SNOWFLAKE_ACCOUNT}
      SNOWFLAKE_USER: ${SNOWFLAKE_USER}
      SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD}
      SNOWFLAKE_ROLE: ${SNOWFLAKE_ROLE}
      SNOWFLAKE_WAREHOUSE: ${SNOWFLAKE_WAREHOUSE}
      SNOWFLAKE_DATABASE: ${SNOWFLAKE_DATABASE}
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./aws_infrastructure:/opt/project/aws_infrastructure
      - ./dbt:/opt/project/dbt
    command: >
      bash -c "
      airflow db init &&
      airflow scheduler &
      airflow webserver
      "
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s

  terraform:
    image: hashicorp/terraform:1.6.7
    container_name: coretelecoms-terraform
    working_dir: /workspace
    volumes:
      - ./aws_infrastructure:/workspace
    entrypoint: ["tail", "-f", "/dev/null"]
    tty: true

  dbt:
    image: coretelecoms-etl:latest
    container_name: coretelecoms-dbt
    working_dir: /opt/project/dbt
    volumes:
      - ./dbt:/opt/project/dbt
    entrypoint: ["tail", "-f", "/dev/null"]
    tty: true
    environment:
      SNOWFLAKE_ACCOUNT: ${SNOWFLAKE_ACCOUNT}
      SNOWFLAKE_USER: ${SNOWFLAKE_USER}
      SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD}
      SNOWFLAKE_ROLE: ${SNOWFLAKE_ROLE}
      SNOWFLAKE_WAREHOUSE: ${SNOWFLAKE_WAREHOUSE}
      SNOWFLAKE_DATABASE: ${SNOWFLAKE_DATABASE}

volumes:
  airflow_logs:
  terraform_state:
